{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Projekt-Notebook-Sabrina.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPEZnUrlRUak3DssWutfq8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saludwig26/Data-science-Team-5/blob/master/azubi_evaluation_uni_mannheim/code/Projekt_Notebook_Sabrina.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUz3CufufiyG",
        "outputId": "8855d604-f4d1-4cde-f4db-f8bc59430135"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "%cd drive/MyDrive\n",
        "%cd ML TensorFlow\n",
        "%cd azubi_evaluation_uni_mannheim\n",
        "%cd data"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/ML TensorFlow\n",
            "/content/drive/MyDrive/ML TensorFlow/azubi_evaluation_uni_mannheim\n",
            "/content/drive/MyDrive/ML TensorFlow/azubi_evaluation_uni_mannheim/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcnS4nrmjaSG"
      },
      "source": [
        "#import csv\n",
        "#with open('data_MailsAndPC_cleaned.csv', 'r') as file:\n",
        "#    reader = csv.reader(file)\n",
        "#    for row in reader:\n",
        "#        print(row)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W37R846q3gXB"
      },
      "source": [
        "#import io\n",
        "#azubi_df = pd.read_csv('data_MailsAndPC_cleaned.csv')\n",
        "#print(azubi_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oH89RUG65ajZ",
        "outputId": "25285651-6759-479b-c1a8-c7f25922fd05"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "azubi_df = pd.read_csv('data_MailsAndPC.csv')\n",
        "print(azubi_df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     Unnamed: 0      id  ... a4s3i4a a4s3i4b\n",
            "0            60  111015  ...     1.0    19.0\n",
            "1            61  111023  ...     5.0    21.0\n",
            "2            62  111036  ...     2.0    40.0\n",
            "3            63  111049  ...     7.0    29.0\n",
            "4            64  111051  ...     0.0     5.0\n",
            "..          ...     ...  ...     ...     ...\n",
            "775         863  631294  ...     0.0    51.0\n",
            "776         864  631306  ...     1.0    45.0\n",
            "777         865  631319  ...     3.0    45.0\n",
            "778         866  631321  ...     3.0    20.0\n",
            "779         867  631334  ...     0.0    33.0\n",
            "\n",
            "[780 rows x 43 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-sJIisD51IB"
      },
      "source": [
        "azubi_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBy627JQ5-5R"
      },
      "source": [
        "#Christopher's cleaning to 548\n",
        "def dfreplace(df, *args, **kwargs):\n",
        "    s = pd.Series(df.values.flatten())\n",
        "    s = s.str.replace(*args, **kwargs)\n",
        "    return pd.DataFrame(s.values.reshape(df.shape), df.index, df.columns)\n",
        "\n",
        "def cleanupDataframe(azubi_df):\n",
        "    #Remove unneccesary columns\n",
        "    azubi_df = azubi_df.drop(columns=['Unnamed: 0', 'id'])\n",
        "    #Remove \"cr\". Assumption made, that there are no real words containing cr in the mails.\n",
        "    azubi_df[\"S1_mail\"] = azubi_df[\"S1_mail\"].str.replace(\"cr\", \"\")\n",
        "    azubi_df[\"S2_mail\"] = azubi_df[\"S2_mail\"].str.replace(\"cr\", \"\")\n",
        "    azubi_df[\"S3_mail\"] = azubi_df[\"S3_mail\"].str.replace(\"cr\", \"\")\n",
        "    #Drop NA rows to only have full datasets\n",
        "    azubi_df = azubi_df.dropna()\n",
        "    #Select all rows without '99' | ~ inverts the operator\n",
        "    dropThisNumber = 99\n",
        "    azubi_df = azubi_df[~azubi_df.eq(dropThisNumber).any(1)]\n",
        "    #Reset the index of the Dataframe\n",
        "    azubi_df = azubi_df.reset_index(drop=True)\n",
        "    return azubi_df\n",
        "\n",
        "def makeList(azubi_df):\n",
        "  azubi_df_list = azubi_df.values.tolist()\n",
        "\n",
        "  #Sentence S1\n",
        "  s1MailSentences = azubi_df[\"S1_mail\"].tolist()\n",
        "  #Labels S1\n",
        "  s1MailA3Labels = azubi_df[\"a3s1\"].tolist()\n",
        "  s1MailA4Labels = azubi_df[\"a4s1\"].tolist()\n",
        "  #Sentence S2\n",
        "  s2MailSentences = azubi_df[\"S2_mail\"].tolist()\n",
        "  #Labels S2\n",
        "  s2MailA3Labels = azubi_df[\"a3s2\"].tolist()\n",
        "  s2MailA4Labels = azubi_df[\"a4s2\"].tolist()\n",
        "  #Sentence S3\n",
        "  s3MailSentences = azubi_df[\"S3_mail\"].tolist()\n",
        "  #Labels S3\n",
        "  s3MailA3Labels = azubi_df[\"a3s3\"].tolist()\n",
        "  s3MailA4Labels = azubi_df[\"a4s3\"].tolist()\n",
        "\n",
        "  return azubi_df_list , s1MailSentences, s1MailA3Labels, s1MailA4Labels, s2MailSentences, s2MailA3Labels, s2MailA4Labels, s3MailSentences, s3MailA3Labels, s3MailA4Labels\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7oMoQeWMlf_"
      },
      "source": [
        "azubi_df = cleanupDataframe(azubi_df)\n",
        "azubi_df\n",
        "\n",
        "azubi_df_list , s1MailSentences, s1MailA3Labels, s1MailA4Labels, s2MailSentences, s2MailA3Labels, s2MailA4Labels, s3MailSentences, s3MailA3Labels, s3MailA4Labels = makeList(azubi_df)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3-YnIE_6UAk"
      },
      "source": [
        "azubi_df #548 participants #after cleaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_HP6Xm9Bwjy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b3e24f-3a03-4322-d38c-3c1cd2c36881"
      },
      "source": [
        "#save cleaned df to csv 'azubi_dfnew.csv'\n",
        "pd.DataFrame.to_csv(azubi_df)\n",
        "print(azubi_df)\n",
        "dfnew = pd.DataFrame(azubi_df)\n",
        "dfnew.to_csv('azubi_dfnew.csv', header=False, index=False)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                                               S1_mail  ... a4s3i4b\n",
            "0    Hallo Fr. Meier,leider konnte ich die von Ihne...  ...    19.0\n",
            "1    Ihre Antwort...Der EP Plan weicht viel zusehr ...  ...    21.0\n",
            "2    bitte kommen Sie rüber und erklären mir es selbst  ...    40.0\n",
            "3    Sehr geehrte Frau Meier,leider konnte Ich Ihre...  ...    29.0\n",
            "4    Hallo Frau Meier,das Controlling hat für das J...  ...    75.0\n",
            "..                                                 ...  ...     ...\n",
            "543  Hallo Frau Meier, ich habe die Aufgabe so gut ...  ...    35.0\n",
            "544  Hallo Frau Meier,ich habe mich an der Aufgabe ...  ...    45.0\n",
            "545  Ihre Antwort...Hallo Frau Meier,leider habe ic...  ...    45.0\n",
            "546  Guten Tag Frau Susanne Meier,ich habe bereits ...  ...    20.0\n",
            "547  Hallo Frau Meier,anbei sende ich Ihnen die Tab...  ...    33.0\n",
            "\n",
            "[548 rows x 41 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HoNNesjsx2KF"
      },
      "source": [
        "#importing packages \n",
        "import tensorflow as tf\n",
        "import csv\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "#Global Hyperparameters\n",
        "vocab_size = 5000\n",
        "embedding_dim = 16\n",
        "max_length = 120\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\""
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grq5C3ceFezA",
        "outputId": "54e127e3-ab99-4efe-b383-d234a54f1160"
      },
      "source": [
        "#stopwords\n",
        "import nltk \n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stemmer = SnowballStemmer(\"german\")\n",
        "stop_words = set(stopwords.words(\"german\"))\n",
        "\n",
        "print(stop_words)\n",
        "\n",
        "#allerdings Wörter wie \"nicht\" enthalten"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "{'viel', 'demselben', 'allen', 'ihrem', 'seinen', 'deines', 'nur', 'wieder', 'des', 'alles', 'im', 'welchen', 'euren', 'dann', 'sie', 'auch', 'dass', 'hat', 'eurem', 'anderen', 'anderer', 'bin', 'dessen', 'hier', 'hinter', 'daß', 'kann', 'meines', 'unter', 'welche', 'dieser', 'jenem', 'zu', 'anderem', 'anders', 'einer', 'vor', 'jenen', 'könnte', 'auf', 'oder', 'hatte', 'gegen', 'keine', 'solchem', 'selbst', 'seine', 'einmal', 'meinen', 'wollte', 'deinem', 'jedem', 'die', 'für', 'derselben', 'jener', 'ihr', 'man', 'ihre', 'werden', 'aber', 'aller', 'seines', 'es', 'dieselben', 'jeden', 'seinem', 'warst', 'jedes', 'über', 'er', 'in', 'meine', 'unsere', 'habe', 'sehr', 'ihnen', 'will', 'eine', 'also', 'dazu', 'einen', 'einigem', 'zwischen', 'was', 'bist', 'diesen', 'hatten', 'allem', 'bei', 'können', 'ihrer', 'weg', 'mancher', 'dort', 'du', 'einem', 'euch', 'ander', 'eures', 'keines', 'sein', 'werde', 'machen', 'nun', 'dieselbe', 'manchen', 'welchem', 'von', 'den', 'sonst', 'derer', 'zum', 'deinen', 'deiner', 'anderm', 'weil', 'denn', 'eines', 'doch', 'wie', 'einige', 'unseres', 'meiner', 'unseren', 'wir', 'nichts', 'jeder', 'dieses', 'einiger', 'an', 'jetzt', 'zur', 'nicht', 'nach', 'der', 'ihn', 'anderes', 'ihm', 'derselbe', 'mit', 'sich', 'soll', 'keinen', 'manches', 'ins', 'so', 'vom', 'als', 'bis', 'unserem', 'dein', 'manche', 'sind', 'welches', 'weiter', 'am', 'mein', 'meinem', 'denselben', 'einigen', 'wenn', 'durch', 'ob', 'alle', 'gewesen', 'anderr', 'haben', 'jede', 'andern', 'damit', 'einiges', 'keiner', 'sollte', 'würden', 'dies', 'keinem', 'zwar', 'hab', 'solche', 'wollen', 'solchen', 'waren', 'welcher', 'diese', 'mir', 'um', 'ohne', 'sondern', 'dir', 'ist', 'kein', 'diesem', 'mich', 'jenes', 'ein', 'deine', 'euer', 'und', 'ich', 'dich', 'wirst', 'seiner', 'wo', 'eure', 'würde', 'das', 'während', 'musste', 'desselben', 'dasselbe', 'ihres', 'wird', 'noch', 'ihren', 'hin', 'eurer', 'etwas', 'einig', 'jene', 'indem', 'uns', 'da', 'solcher', 'muss', 'war', 'manchem', 'dem', 'unser', 'solches', 'aus', 'andere'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yenCuvukbDIZ"
      },
      "source": [
        "#creating lists by putting the sentences and labels into an array #placing the sequences in square brackets \n",
        "\n",
        "s1MailSentences = []\n",
        "s1MailA3Labels = []\n",
        "s1MailA4Labels = [] \n",
        "\n",
        "s2MailSentences = [] \n",
        "s2MailA3Labels = [] \n",
        "s2MailA4Labels = [] \n",
        "\n",
        "s3MailSentences = []\n",
        "s3MailA3Labels = [] \n",
        "s3MailA4Labels = []\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k7wJCV67XGLW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a66e0cf9-eac6-42df-c660-b04e527e1460"
      },
      "source": [
        "with open(\"azubi_dfnew.csv\") as csvfile:\n",
        "    reader = csv.reader(csvfile, delimiter=',')\n",
        "    next(reader)\n",
        "    for row in reader:\n",
        "        s1MailSentences = row[0]\n",
        "        s2MailSentences = row[1] \n",
        "        s3MailSentences = row[2] \n",
        "        s1MailA3Labels.append(row[3]) #elements in lists\n",
        "        s2MailA3Labels.append(row[4])\n",
        "        s3MailA3Labels.append(row[5])\n",
        "        s1MailA4Labels.append(row[6]) \n",
        "        s2MailA4Labels.append(row[7])\n",
        "        s3MailA4Labels.append(row[8])\n",
        "\n",
        "        #for word in stopwords:\n",
        "        #    token = \" \" + word + \" \" #word =stopword gets tokenized\n",
        "        #    sentence = sentence.replace(token, \" \") #replace token with space\n",
        "        #    sentence = sentence.replace(\"  \", \" \") #to avoid the creation of non-existing words?\n",
        "        #sentences.append(sentence) #adding sentence to thte list sentences which contain at the end 2225 items\n",
        "\n",
        "\n",
        "#print(s1MailSentences)\n",
        "print(row[0])\n",
        "print(row[1])\n",
        "print(row[2])\n",
        "print(row[3])\n",
        "print(row[4])\n",
        "print(row[5])\n",
        "print(len(s3MailA3Labels))\n",
        "print(s3MailA3Labels)\n",
        "print(len(s3MailSentences)) #Output: 217? and 548 items\n",
        "print(s3MailSentences[0])"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hallo Frau Meier,anbei sende ich Ihnen die Tabelle zurück.Mir ist es gelungen, die Abweichungen der Soll- und Ist-Werte und auch die Sollwerte der letzten 3 Lieferanten zu berechnen.Abweichungen mit größer/gleich 20 % habe ich farbig markiert.Leider konnte ich keine Gründe für diese Abweichungen finden.Mein Vorschlag wäre allerdings die geplante Absatzmenge aufgrund des realisierten Absatzes anzupassen.Ebenfalls würde ich einen Lieferantenvergleich vorschlagen, um durch günstigere Lieferanten eventuell Kosten einzusparen und somit die Planmengen zu erreichen. GrüßeMelissa Fischer \n",
            "Hallo Herr Neumann,anbei erhalten Sie die Tabelle der Nutzwertanalyse zurück.Laut dieser Tabelle sollten wir wolrd birdy bei dem Lieferanten aus der Tschechei beziehen, da dieser Punktemäsig weit vorne liegt und auch in puncto Qualität und Preis-Leistungsverhältnis überzeugt.Leider kann ich Ihnen keine weiteren Infos liefern, da ich mehr Zeit benötigt hätte.GRüßeMelidssa Fischer \n",
            "Hallo Herr Neumann,anbei die ausfüllten Tabellen.Ich bin dadurch zu dem Schluss gekommen, dass ein Fremdbezug ein wenig günstiger ist als Eigenfertigung.Leider kann ich nicht mehr Infos liefern. Grüße Melissa fischer \n",
            "2.0\n",
            "3.0\n",
            "2.0\n",
            "547\n",
            "['0.0', '0.0', '0.0', '2.0', '0.0', '4.0', '3.0', '0.0', '2.0', '2.0', '0.0', '0.0', '4.0', '0.0', '2.0', '0.0', '2.0', '0.0', '4.0', '3.0', '3.0', '2.0', '3.0', '3.0', '3.0', '3.0', '3.0', '4.0', '2.0', '0.0', '5.0', '0.0', '3.0', '1.0', '0.0', '0.0', '0.0', '3.0', '2.0', '3.0', '2.0', '5.0', '4.0', '5.0', '3.0', '2.0', '0.0', '0.0', '0.0', '2.0', '2.0', '2.0', '4.0', '2.0', '2.0', '2.0', '0.0', '3.0', '4.0', '0.0', '4.0', '4.0', '2.0', '0.0', '3.0', '3.0', '3.0', '2.0', '0.0', '2.0', '2.0', '0.0', '4.0', '0.0', '0.0', '2.0', '3.0', '0.0', '0.0', '2.0', '3.0', '0.0', '4.0', '2.0', '2.0', '3.0', '3.0', '2.0', '0.0', '0.0', '3.0', '2.0', '3.0', '2.0', '0.0', '2.0', '3.0', '0.0', '0.0', '0.0', '0.0', '0.0', '2.0', '2.0', '4.0', '2.0', '1.0', '3.0', '0.0', '3.0', '2.0', '3.0', '4.0', '3.0', '3.0', '0.0', '3.0', '0.0', '0.0', '0.0', '1.0', '3.0', '3.0', '0.0', '2.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '2.0', '4.0', '0.0', '0.0', '0.0', '0.0', '2.0', '0.0', '3.0', '4.0', '1.0', '0.0', '3.0', '0.0', '0.0', '3.0', '0.0', '0.0', '0.0', '2.0', '0.0', '0.0', '0.0', '2.0', '4.0', '0.0', '0.0', '3.0', '5.0', '3.0', '0.0', '0.0', '3.0', '0.0', '0.0', '0.0', '3.0', '0.0', '3.0', '0.0', '0.0', '0.0', '3.0', '3.0', '3.0', '1.0', '3.0', '4.0', '4.0', '2.0', '3.0', '5.0', '0.0', '0.0', '2.0', '2.0', '3.0', '0.0', '3.0', '4.0', '4.0', '0.0', '0.0', '4.0', '1.0', '0.0', '0.0', '3.0', '2.0', '0.0', '1.0', '0.0', '3.0', '0.0', '1.0', '0.0', '2.0', '0.0', '0.0', '0.0', '3.0', '3.0', '0.0', '0.0', '2.0', '3.0', '4.0', '2.0', '3.0', '4.0', '0.0', '3.0', '3.0', '2.0', '2.0', '0.0', '3.0', '0.0', '3.0', '0.0', '2.0', '0.0', '0.0', '3.0', '3.0', '0.0', '0.0', '0.0', '0.0', '2.0', '3.0', '0.0', '2.0', '4.0', '3.0', '0.0', '2.0', '0.0', '3.0', '4.0', '4.0', '2.0', '3.0', '3.0', '2.0', '2.0', '1.0', '0.0', '3.0', '1.0', '1.0', '0.0', '0.0', '0.0', '4.0', '0.0', '2.0', '1.0', '3.0', '3.0', '3.0', '0.0', '0.0', '3.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '2.0', '4.0', '0.0', '3.0', '0.0', '2.0', '2.0', '2.0', '3.0', '1.0', '4.0', '0.0', '2.0', '3.0', '4.0', '0.0', '2.0', '0.0', '3.0', '3.0', '2.0', '0.0', '0.0', '0.0', '3.0', '3.0', '0.0', '0.0', '2.0', '0.0', '3.0', '3.0', '3.0', '3.0', '3.0', '3.0', '1.0', '3.0', '5.0', '3.0', '2.0', '3.0', '5.0', '3.0', '2.0', '4.0', '0.0', '0.0', '2.0', '3.0', '3.0', '0.0', '3.0', '2.0', '0.0', '2.0', '3.0', '0.0', '3.0', '0.0', '3.0', '4.0', '3.0', '1.0', '3.0', '3.0', '2.0', '3.0', '0.0', '5.0', '4.0', '4.0', '3.0', '3.0', '0.0', '0.0', '3.0', '0.0', '2.0', '3.0', '0.0', '0.0', '0.0', '0.0', '3.0', '2.0', '0.0', '0.0', '0.0', '2.0', '0.0', '0.0', '2.0', '0.0', '0.0', '3.0', '0.0', '1.0', '0.0', '0.0', '2.0', '3.0', '3.0', '1.0', '4.0', '2.0', '0.0', '2.0', '3.0', '3.0', '4.0', '1.0', '0.0', '3.0', '0.0', '3.0', '4.0', '2.0', '3.0', '1.0', '4.0', '3.0', '5.0', '2.0', '3.0', '2.0', '3.0', '2.0', '2.0', '0.0', '0.0', '1.0', '0.0', '0.0', '3.0', '3.0', '0.0', '4.0', '0.0', '0.0', '2.0', '1.0', '0.0', '0.0', '3.0', '0.0', '0.0', '0.0', '3.0', '2.0', '0.0', '2.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '2.0', '4.0', '2.0', '2.0', '4.0', '2.0', '2.0', '0.0', '2.0', '3.0', '0.0', '3.0', '2.0', '1.0', '0.0', '0.0', '0.0', '3.0', '4.0', '2.0', '2.0', '0.0', '0.0', '0.0', '0.0', '2.0', '0.0', '4.0', '0.0', '0.0', '1.0', '3.0', '2.0', '3.0', '4.0', '1.0', '2.0', '3.0', '0.0', '3.0', '3.0', '3.0', '0.0', '2.0', '0.0', '3.0', '0.0', '0.0', '0.0', '1.0', '0.0', '0.0', '0.0', '2.0', '1.0', '0.0', '1.0', '2.0', '0.0', '2.0', '5.0', '2.0', '0.0', '0.0', '0.0', '3.0', '3.0', '0.0', '2.0', '0.0', '4.0', '1.0', '3.0', '0.0', '0.0', '1.0', '2.0', '3.0', '3.0', '3.0', '3.0', '4.0', '3.0', '2.0', '2.0', '4.0', '3.0', '2.0', '0.0', '3.0', '3.0', '0.0', '0.0', '0.0', '0.0', '0.0', '0.0', '2.0', '1.0', '2.0', '0.0', '0.0', '0.0', '2.0', '0.0', '2.0', '2.0']\n",
            "217\n",
            "H\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Az5kKo_DPdOu"
      },
      "source": [
        "azubi_df_list , s1MailSentences, s1MailA3Labels, s1MailA4Labels, s2MailSentences, s2MailA3Labels, s2MailA4Labels, s3MailSentences, s3MailA3Labels, s3MailA4Labels = makeList(azubi_df)\n",
        "azubi_df_list \n",
        "\n",
        "#azubi2_df = []\n",
        "#s1MailSentences, s1MailA3Labels, s1MailA4Labels, s2MailSentences, s2MailA3Labels, s2MailA4Labels, s3MailSentences, s3MailA3Labels, s3MailA4Labels = makeList(azubi2_df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wxJbSHdXIDh",
        "outputId": "c03fb8b0-c707-4648-8eb6-dc274165a0fe"
      },
      "source": [
        "#merging all Mails together into a list\n",
        "Sentences = s1MailSentences + s2MailSentences + s3MailSentences \n",
        "Sentences\n",
        "print(len(Sentences))\n",
        "#Output: 3 tasks x 548 mails= 1644 mails"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1644\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6S_02x-_ZMoL",
        "outputId": "c25e0391-40bf-40c4-cc09-c2fa004c4dcd"
      },
      "source": [
        "#merging all Labels together into a list\n",
        "Labels = s1MailA3Labels + s1MailA4Labels + s2MailA3Labels + s2MailA4Labels + s3MailA3Labels + s3MailA4Labels\n",
        "Labels\n",
        "print(len(Labels))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3288\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI1sF96bTno0",
        "outputId": "d239f067-0537-42e2-e795-001c79a6e70c"
      },
      "source": [
        "#whole sample 1644\n",
        "train_size = 1194     #548*3 emails\n",
        "test_size = 225       #75*3 emails\n",
        "validation_size = 225 #75*3 emails\n",
        "\n",
        "print(train_size)\n",
        "print(test_size)\n",
        "print(validation_size)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1194\n",
            "225\n",
            "225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJIgCcRfSgbV"
      },
      "source": [
        "def trainDevTestSplit(Sentences, train_size):\n",
        "    random.shuffle(Sentences)\n",
        "    for x in range(train_size):\n",
        "        Sentences.append(Sentences[x][0])\n",
        "        Labels.append(Sentences[x][0]) \n",
        "\n",
        "    train_sentences = Sentences[0:1194]\n",
        "    train_labels = Labels[0:1194]\n",
        "    test_sentences = Sentences[1194:1419]\n",
        "    test_labels = Labels[1194:1419]\n",
        "    validation_sentences = Sentences[1419:1644]\n",
        "    validation_labels = Labels[1419:1644]\n",
        "    return train_sentences, train_labels, validation_sentences, validation_labels, test_sentences, test_labels\n",
        "\n",
        "#print(train_size)\n",
        "#print(len(train_sentences))\n",
        "#print(train_sentences)\n",
        "#print(len(train_labels))\n",
        "\n",
        "#print(len(validation_sentences))\n",
        "#print(len(validation_labels))\n",
        "\n",
        "#print(len(test_sentences))\n",
        "#print(len(test_labels))\n",
        "\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QvilD-l7UzuZ"
      },
      "source": [
        "train_sentences, train_labels, validation_sentences, validation_labels, test_sentences, test_labels = trainDevTestSplit(Sentences, train_size)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uujEuMmSP_aD"
      },
      "source": [
        "train_sentences #hier sieht man die einzelnen Buchstaben wie 'H'\n",
        "#print(len(train_sentences))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZbg9vnHN9Qa"
      },
      "source": [
        "#tokenize data\n",
        "def tokenizeData(vocab_size, oov_tok, train_sentences, max_length, padding_type, trunc_type):\n",
        "    tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "   \n",
        "    word_index = tokenizer.word_index\n",
        "    vocab_size=len(word_index)\n",
        "    \n",
        "    sequences = tokenizer.texts_to_sequences(train_sentences)\n",
        "    padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "    return padded\n",
        "\n",
        "#print(len(word_index))\n",
        "#print(word_index)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7cqBA-_Qvny"
      },
      "source": [
        "padded = tokenizeData(vocab_size, oov_tok, train_sentences, max_length, padding_type, trunc_type)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "equ6Ay48Q5D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54e2e30a-03a1-4f0d-827c-79bd02e507af"
      },
      "source": [
        "padded"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 13,  16,  17, ...,   0,   0,   0],\n",
              "       [ 13,  16,  17, ...,   0,   0,   0],\n",
              "       [  6,   0,   0, ...,   0,   0,   0],\n",
              "       ...,\n",
              "       [  6,   0,   0, ...,   0,   0,   0],\n",
              "       [ 34, 165,  40, ...,   0,   0,   0],\n",
              "       [ 13,  16,  17, ...,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mhH6egqqVpzF"
      },
      "source": [
        "validation_sentences "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "GOQqRPtSWPGM",
        "outputId": "f29cf1fb-3565-485e-e1a2-23e776fb289e"
      },
      "source": [
        "validation_sequences = Tokenizer.texts_to_sequences(validation_sentences) #liegt die Fehlermeldung vielleicht an den einzelnen Buchstaben?\n",
        "validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "print(len(validation_sequences))\n",
        "print(validation_padded.shape)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-a3ae1cc7c6fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mvalidation_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvalidation_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: texts_to_sequences() missing 1 required positional argument: 'texts'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16o9hxKymW1i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "248e6386-1fc5-4aa2-d3ea-53482559346e"
      },
      "source": [
        "test_sequences = Tokenizer.texts_to_sequences(test_sentences)\n",
        "test_padded = pad_sequences(test_sequences, padding=padding_type, maxlen=max_length)\n",
        "\n",
        "print(len(test_sequences))\n",
        "print(test_padded.shape)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-5596726eb02a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sentences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_padded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: texts_to_sequences() missing 1 required positional argument: 'texts'"
          ]
        }
      ]
    }
  ]
}